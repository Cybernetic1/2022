\input{../YKY-preamble.tex}
\setmainfont[BoldFont=Alibaba_Sans_Regular.otf,ItalicFont=Alibaba_Sans_Light_Italic.otf]{Alibaba_Sans_Light.otf}
	
\usepackage[active,tightpage]{preview}		% for continuous page(s)
\renewcommand{\PreviewBorder}{0.5cm}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}

\usepackage[absolute,overlay]{textpos}		% for page number on upper left corner

\usepackage{color}
\usepackage{mathtools}
\usepackage[hyperfootnotes=false]{hyperref}

% \usepackage[backend=biber,style=numeric]{biblatex}
% \bibliography{../AGI-book}
% \renewcommand*{\bibfont}{\footnotesize}

\usetikzlibrary{shapes}
\usepackage[export]{adjustbox}				% ??
\usepackage{verbatim} % for comments
% \usepackage{newtxtext,newtxmath}	% Times New Roman font

% \numberwithin{equation}{subsection}

\newcommand{\underdash}[1]{%
	\tikz[baseline=(toUnderline.base)]{
		\node[inner sep=1pt,outer sep=10pt] (toUnderline) {#1};
		\draw[dashed] ([yshift=-0pt]toUnderline.south west) -- ([yshift=-0pt]toUnderline.south east);
	}%
}%


%\DeclareSymbolFont{symbolsC}{U}{txsyc}{m}{n}
%\DeclareMathSymbol{\strictif}{\mathrel}{symbolsC}{74}
\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
% \setmathfont{Latin Modern Math}
\DeclareMathOperator*{\argmin}{arg\,min}

% \usepackage[most]{tcolorbox}
\tcbset{on line, 
	boxsep=4pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
	colframe=red,colback=pink,
	highlight math style={enhanced}
}
\newcommand{\atom}{\vcenter{\hbox{\tcbox{....}}}}

% \newcommand{\emp}[1]{{\color{violet}\textbf{#1}}}
\let\oldtextbf\textbf
\renewcommand{\textbf}[1]{\textcolor{blue}{\oldtextbf{#1}}}

\newcommand{\logic}[1]{{\color{violet}{\textit{#1}}}}
\newcommand*\smileFace{$\vcenter{\hbox{\includegraphics[scale=0.6]{../smiley.jpg}}}$}
\newcommand{\underconst}{\includegraphics[scale=0.5]{../2020/UnderConst.png}}
\newcommand{\KBsymbol}{\vcenter{\hbox{\includegraphics[scale=1]{../KB-symbol.png}}}}
\newcommand{\witness}{\scalebox{0.6}{$\blacksquare$}}
% \newcommand{\Heytingarrow}{\mathrel{-}\mathrel{\triangleright}}
% \providecommand\Heytingarrow{\relbar\joinrel\mathrel{\vcenter{\hbox{\scalebox{0.75}{$\rhd$}}}}}

\begin{document}

\begin{preview}

\cc{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\Large 逻辑与深度学习的关系}}
}{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\Large Comparison of Logic AI and Deep Learning}}
}

% \author{YKY} % Your name
\date{\vspace{-2cm}} % Date, can be changed to a custom date

\maketitle

\setcounter{section}{-1}

% (1) Circled page number on upper left corner
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
{\color{red}{\large \textcircled{\small 1}}}
\end{textblock*}

\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

这是经典逻辑 AI 的最基本运作模式：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{LBAI-basic-config.png}}}
\end{equation}

它其实包含了两个算法：
\begin{itemize}
	\item \textbf{matching} (unification): \\
	逻辑 rules 是包含变量的条件命题， \\
	例如 \tab \logic{$\forall x. \mbox{是人}(x) \Rightarrow \mbox{会死}(x). $ }\\
	Unification 判定一条 rule 是否可以 apply 到某逻辑命题上，\\
	例如：\logic{是人(苏格拉底)} 可以跟上式的左边 unify. \\
	Matching 的结果是得到一推 instantiated（特例化，即不包含变量）的命题。
	
	\item \textbf{forward- or backward-chaining} (resolution): \\
	由已知事实 推导出新结论，或反过来，判断某给定的新结论是否成立。 \\
	例如：\logic{ 是人(苏格拉底) $\Rightarrow$ 会死(苏格拉底) $\;\; \wedge$ 是人(苏格拉底) } \\
	可以推出：\logic{会死(苏格拉底)}。
\end{itemize}

深度学习的特点，就是将
\begin{equation}
\mbox{状态}_t  \vdash \mbox{状态}_{t+1}
\end{equation}
的逻辑推导过程，通通纳入进去一个非常复杂的非线性函数（= 深度神经网络）里面。 这样做以后，上述的逻辑结构被
``mingled'' 在一起，以至于很难分辨了。 但也正是由于这种「大杂烩」，深度神经网络 将一套复杂的组合算法 压缩成数量不算太多的一层层的参数。 它同时可以做 learning 和 inference 这两个动作。 这种简单粗暴的方法，其实非常有效率，要超越它的速度并不容易！

我们知道（或推测）一个智能系统 应该具有 符号逻辑的结构。 这点知识可不可以用来 约束/加速 深度神经网络？ 答案似乎是有可能的。 现时 state-of-the-art 处理 视觉的 CNN 和 处理文字的 BERT，它们都有内部结构， \textbf{而不是 fully-connected}，而且 这内部结构 对应于 被处理的资料的结构。 因此我们有理由相信，逻辑结构 可以用来约束 深度神经网络的结构，达到加速。 

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 2}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{逻辑与深度学习}{Logic and Deep Learning}}
\end{textblock*}
\vspace*{0.3cm} 

接下来我们详细一点看逻辑系统的结构：

Knowledge Base 里面有很多 rules，系统要将这些 rules 逐一 match with 系统状态 (= working memory) 里面的命题：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{rete-explained-1.png}}}
\end{equation}
成功 matched 的 rules 可以导出新的结论，加进 working memory 的状态 里面。

这个复杂的操作，完全被一个神经网络取代。 或者可以更抽象地说：
\begin{equation}
\label{eqn:some-kind-of-memory}
\vcenter{\hbox{\includegraphics[scale=1]{some-kind-of-memory.png}}}
\end{equation}

而以 Transformer 来说，它是一种 输入元 \textbf{之间} 的记忆体（这记忆就储存在 Q, K, V 矩阵里），而它 \textbf{implicitly} 做到了 rules 的作用：
\begin{equation}
\label{fig:self-attention-as-Rete}
\vcenter{\hbox{\includegraphics[scale=0.9]{rete-explained-3b.png}}}
\end{equation}
换句话说，Transformer 内部有某种（扭曲了的）逻辑 rules 的结构。 那么很自然的问题就是：能否发掘更多 逻辑／逻辑系统 的结构？ 也就是说，公式 (\ref{eqn:some-kind-of-memory}) 可以有怎样的代数结构约束？ 这个问题 可以参考 范畴逻辑 的理论，还有 经典 logic-based AI 系统的理论。

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
	
\setlength{\parskip}{0.4\baselineskip}
\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 3}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{逻辑与深度学习}{Logic and Deep Learning}}
\end{textblock*}
\vspace*{0.3cm} 

我们希望 勾画出公式 (\ref{eqn:some-kind-of-memory}) 需要具备的代数约束，但暂时先用文字描述比较容易：

\begin{itemize}
	\item 状态是 \textbf{颗粒化} 的，它是某集合的元素，元素之间可交换，也就是 Transformer 的 equivariance. （注意： Transformer 有 equivariance，但 equivariance 未必一定要用 Transformer 实现）

	\item \textbf{深度结构}：例如多层网络，每层是函数的复合 (composition). Transformer 也用了深度结构。
	
	\item 逻辑 包括了 \textbf{命题}层次 和 \textbf{命题内部}层次 的 颗粒化。 后者是\textbf{谓词} (predicate) 逻辑的结构，例如： \logic{loves( John, Mary )}，也可以简单地将它视为 \textbf{代数元}之间的\textbf{乘积}，例如： \logic{John \bullet\  loves \bullet\   Mary}, 后者也叫做 ``word''.  （不同类别的代数元之间不一定容许乘积，因此有 groupoid 的概念，但暂时来说这细节不重要。） 现时重点是如何将 这两层的 颗粒化 结构 施加到深度神经网络上。 

	\item 逻辑推导 每步只产生\textbf{一个}新的结论（或其概率分布），然后这个新的结论，再加入到旧的状态中，作为一个命题集合的元素，而旧状态也要 \textbf{遗忘} 一些命题，否则需要无限记忆。 这跟 Transformer 每次输出\textbf{一列}的 tokens 有点不同（虽然我们不太肯定 Transformer tokens 究竟对应于 命题 还是 谓词／原子概念）。 
	
	\item 逻辑 rule 通常只跟某几个前提有关，其它前提是\textbf{无关}的，例如： \logic{眼睛好看 $\wedge$ 鼻子好看 $\wedge$ 嘴巴好看 $\Rightarrow$ 帅}，跟 \logic{有钱} 或 \logic{穷} 无关。 Transformer 的 \textbf{softmax} 结构似乎也可以排除一些无关的 tokens 的影响。

	\item （可能还有其他的结构特征.....）
\end{itemize}

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}

\setlength{\parskip}{0.4\baselineskip}
\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 4}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{逻辑与深度学习}{Logic and Deep Learning}}
\end{textblock*}
\vspace*{0.3cm} 

根据我的理论，理想的逻辑形式是这样的（各种元素的个数纯粹示意）：
\begin{equation}
\label{fig:logic-symmetry-predicate-level}
\vcenter{\hbox{\includegraphics[scale=1]{logic-symmetry-predicate-level.png}}}
\end{equation}

相比之下，我们目前可以写出来的 代数关系 $p \wedge q = p \wedge q$ 只表达了这种结构：
\begin{equation}
\label{fig:logic-symmetry-propositional-only}
\vcenter{\hbox{\includegraphics[scale=1]{logic-symmetry-propositional-only.png}}}
\end{equation}
相比于 图 (\ref{fig:logic-symmetry-propositional-only})，图 (\ref{fig:logic-symmetry-predicate-level}) 添加了 $\atom$ 的约束，\uline{但这约束怎样用代数表示}？

而 \textbf{Transformer} 处理 命题 和 概念 的方式是这样：
\begin{equation}
\label{fig:logic-symmetry-Transformer}
\vcenter{\hbox{\includegraphics[scale=1]{logic-symmetry-Transformer.png}}}
\end{equation}
它没有 explicit 的命题结构，而是用特别的 token 表示句子的\textbf{终结}，当然还有 positional encoding 这些「伎俩」。 所以，Transformer 是一种比较 \textit{ad hoc} 的设计，我们应该可以改进它。

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}

\setlength{\parskip}{0.4\baselineskip}
\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 5}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{逻辑与深度学习}{Logic and Deep Learning}}
\end{textblock*}
\vspace*{0.3cm} 

现在我们企图回答那\uline{最重要的问题}： 怎样用代数形式表达「命题是由概念原子组成的」？

亦即是说，以下这两个结构的分别在哪里？ 如何用代数表达这不同？
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{logic-symmetry-predicate-level-1.png}}}
\quad \mbox{vs} \quad
\vcenter{\hbox{\includegraphics[scale=0.6]{logic-symmetry-propositional-only-1.png}}}
\end{equation}
这就像问 $ 0...9 \times 0...9 $ 跟 $ 00...99 $ 的分别（基本上没有分别，它们是 isomorphic）。 

类似地，
\begin{equation}
\{ \mbox{ John, Mary } \} \times \{ \mbox{ human, god, worm } \}
\end{equation}
跟
\begin{equation}
\{ \mbox{ John is human }, \mbox{ Mary is human }, .... \}
\end{equation}
等 $2 \times 3 = 6$ 个命题 也是同构的。 但前者是由两组不同的概念结合而成，它的成分可以被 $\forall$ 或 $\exists$ 量化； 后者是 命题逻辑，那些命题是不可分割的，也不可以拆开来量化。 

但由于 $\atom \in$ 非交换自由群（最少结构的群），它没有像 $a \cdot b = b \cdot a$ 那样的对称性公式。

% 似乎要考虑  $\atom$ 的群结构是如何影响 逻辑 rule (\ref{fig:logic-symmetry-predicate-level}) 的输入和输出？ 类似这样：
%\begin{equation}
%q_1 \cdot q_2 \; \Longleftarrow \; p_{11} \cdot p_{12} \; \wedge \; p_{21} \cdot p_{22}
%\end{equation}
%它跟这样有什么不同：
%\begin{equation}
%Q \; \Longleftarrow \; P_1 \; \wedge \; P_2 \quad ?
%\end{equation}

% $Q$ 是由两件东西组成，但这两件东西可以换成另外两件。 但那就不再是同一个 rule.  如果要维持是同一个 rule，则 任何 $\atom$ 元素不可以交换或更换。 那是不是需要考虑所有的 rules？

% 或者可以考虑一个 $\forall$ rule，它包含很多个 instantiated rules.  而它的 substitutions 总是以 群元素为 unit.  这一点可以怎样表示？

% 但其实当 rule 改变时，它似乎也是根据群元素的边界而改变的。 似乎一定要涉及大量的 rules 才可以表达 $\atom$ 的对称性。

% 但从机器学习的角度来看，rules 的更新 似乎没有必要 遵从 $\atom$ 的边界？

% 从经典 AI 的角度看，lattice of rules 结构 就是 algebra of atomic concepts 的结构。

% The proposition structure $P$ is made up of atoms $a$ from a free monoid $A$, and the rule can be quantified by $\forall$ and $\exists$ over such atoms.  ``The rule can be quantified'' means the mapping does not just send one compound element to another element, but that parts of the compound element can be varied and the output element would vary according to the rules of $\forall$ or $\exists$ as adjunctions to substitution maps.

% The \textbf{Atomic Condition} says that the input and output of a rule are made up of atoms that can vary according to $\forall$ and $\exists$ as adjoint functors.

% The \textbf{Atomic Condition} says that each $P_i = a_{i1} ... a_{ik}$ where some $a_{ij}$ \uline{can vary} according to $\forall$ and $\exists$ as adjoint functors.

% The \textbf{Atomic Condition} says that each $P_i = a_{i1} ... a_{ik}$ where some $a_{ij}$ \uline{can be copied} (possibly with some transformation) to another location.  And the transformation has to accord with $\forall$ and $\exists$ as adjunctions.

% The \textbf{Atomic Condition} says that each $P_i = a_{i1} ... a_{ik}$ where some $a_{ij}$ \uline{can be copied} (possibly with some transformation) to another location.  And the transformation has to accord with $\forall$ and $\exists$ as adjunctions.

% The \textbf{Atomic Condition} says that each $P_i = a_{i1} ... a_{iK}$ where some $a_{ih} = T_{\forall}(a_{jk})$ and the transformation $T_{\forall}$ has to accord with $\forall$ as an adjunction to a substitution functor.

经过一番分析之后 我得到了「命题是由概念原子构成的」以下条件：

\begin{tcolorbox}[colback=white, enhanced]
\newtheorem*{condition}{Atomic Condition}
\begin{condition}[AC]
	Each proposition $P_i$ is made up of $K$ atoms:
	\begin{equation}
	P_i = a_{i1} \cdot ... \cdot a_{iK}
	\label{eqn:atomic-condition-1}
	\end{equation}
	where optionally some atoms can be \textbf{copied} to other locations (with a non-linear transformation $\tau$, if they are copied to the output layer) via:
	\begin{equation}
	a_{ih} = a_{jk} \quad \mbox{or} \quad a_{ih} = \tau(a_{jk})
	\label{eqn:atomic-condition-2}
	\end{equation}
	and the transformation $\tau$ has to accord with $\forall$ or $\exists$ as adjunctions to a substitution functor.
\end{condition}
\end{tcolorbox}

其实 $\tau$ 只需要是连续函数，就可以符合上述条件。 所以 Atomic condition 的重点在于 (\ref{eqn:atomic-condition-1}) 和 (\ref{eqn:atomic-condition-2}) 这两个\textbf{等式}，其实是非常简单的。 $\forall$ 和 $\exists$ 作为伴随函子 的范畴论描述 比较复杂，我们会在附录里解释。

那么 等式 (\ref{eqn:atomic-condition-2}) 里面的 ``='' 是来自哪里？ 其实太明显了，它就是逻辑 rule 里面 将变量「syntactically 搬动」的动作：
\vspace{0.5cm}
\begin{equation}
\forall X, Y, Z.  \;\;  \text{grandfather}({\color{red}X} \tikzmark{x}, {\color{red}Z} \tikzmark{z}) \leftarrow \text{father}({\color{red}X} \tikzmark{p}, {\color{red}Y} \tikzmark{y}) \wedge \mbox{father}({\color{red}Y} \tikzmark{q}, {\color{red}Z} \tikzmark{r})
\begin{tikzpicture}[overlay,remember picture,out=45,in=135,distance=1.1cm]
\draw[-,red, transform canvas={shift={(-5pt,18pt)}}] (x.center) to (p.center);
\draw[-,red, transform canvas={shift={(-5pt,18pt)}}] (y.center) to (q.center);
\draw[-,red, transform canvas={shift={(-5pt,-3pt)}}, out=-45,in=225] (z.center) to (r.center);
\end{tikzpicture}
\label{eqn:linkage-father}
\end{equation}
正是 这些「搬动」，构成了「命题是由概念组成的」结构。

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
		
\setlength{\parskip}{0.4\baselineskip}
\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 6}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{逻辑与深度学习}{Logic and Deep Learning}}
\end{textblock*}
\vspace*{0.3cm} 

\textbf{Self-Attention} 的本质 可以这样理解（抽象注意力结构）：
\begin{equation}
\label{fig:abstract-self-attention}
\vcenter{\hbox{\includegraphics[scale=1]{essence-of-self-attention.png}}}
\end{equation}
这垂直的「軸」结构 ({\color{red}红色})，重复在每一条轴上。  所以，当 输入的元素 \textbf{交换}时，输出也随着交换。 这就是 self-Attention 能达到 \textbf{equi-variant} 效果的原因。

而我们想用 类似以上 self-Attention 的方法，解决 逻辑结构 的问题：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{logic-symmetry-logic-Transformer.png}}}
\end{equation}

首先以函数的方式表达 self-Attention 结构：
\begin{eqnarray}
\boxed{\mbox{输出命题 $O_i$ 由原子 $b_i$ 组成}} \quad O_i &=& [b_1 ... b_K] \\
\boxed{\mbox{输入命题 $P_i$ 由原子 $a_i$ 组成}} \quad P_i &=& [a_1 ... a_K] \\
\boxed{\mbox{Self-Attention}} \quad O_i &=& \alpha(P_i \;; P_1 ... \hat{P_i} ... P_N)
\end{eqnarray}
$P_1 ... \hat{P_i} ... P_N$ 的意思是 $P_1 ... P_N$ 除了 $P_i$. \\
$\alpha(P_i \;; ... )$ 是图 (\ref{fig:abstract-self-attention}) 的函数结构，也可以理解为 以 $P_i$ 为 query 的 self Attention.

这里有一个很重要的重点： self-Attention 的前身是来自 Graves \textit{et al.} 的 《Neural Turing Machine》 的 \textbf{content-addressable memory}.

我们要比较 两种做法，前者是简单直接的经典 rule base 结构，后者是以 self-Attention 代替 rule base：
\begin{equation}
\label{fig:rule-base}
\vcenter{\hbox{\includegraphics[scale=1]{shallow-rule-base.png}}}
\end{equation}
\hrule
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.9]{rete-explained-3b.png}}}
\tag{\ref{fig:self-attention-as-Rete}}
\end{equation}
大家要感受到 Transformer 是一种非常 ``twisted'' 的处理 rules matching 的方式。 这个对应很不明显，以至于我们很难分辨出 Transformer 那边的 rules 长什么样子。 然而我觉得 Transformer 的设计者们 或许多少有意识到它跟 rule-based systems 的相似性。 特别地，看看以下这逻辑 rule：
\vspace{0.5cm}
\begin{equation}
\forall X, Y, Z.  \;\;  \text{grandfather}(X \tikzmark{x1}, Z \tikzmark{z1}) \leftarrow \text{father}(X \tikzmark{x2}, {\color{red}Y} \tikzmark{y1}) \wedge \mbox{father}({\color{red}Y} \tikzmark{y2}, Z \tikzmark{z2})
\begin{tikzpicture}[overlay,remember picture,out=45,in=135,distance=1.1cm]
\draw[-, transform canvas={shift={(-5pt,18pt)}}] (x1.center) to (x2.center);
\draw[-,red, transform canvas={shift={(-5pt,18pt)}}] (y1.center) to (y2.center);
\draw[-, transform canvas={shift={(-5pt,-3pt)}}, out=-45,in=225] (z1.center) to (z2.center);
\end{tikzpicture}
\tag{\ref{eqn:linkage-father}}
\end{equation}
这 rule 的前提有两个条件，出现两次的变量 $Y$ 必须相等（{\color{red}红色}），matching 才算成功。 而这种在 rule 的前提内部进行的 \textbf{比较} (comparison) 运作，正是 self-Attention 可以方便地做到的。 但 self-Attention 也忽略了 $A \wedge B$ 的对称性，可能还有改进的空间。

我觉得目前要回答的几个关键问题是：
\begin{itemize}
	\item 根据例如 刘乾 \textit{et al} 的论文，Transformer 做不到某些 逻辑语法上的运作，问题出在哪里？ 似乎不是 Transformer 根本无法学习那种语法，而是 它不能纯粹靠 prompt 做到。 但其实 prompt 是否具有深层意义，还是它只是一个 hack？  我们没有 explicitly「告诉」Transformer 它应该怎么做，那它做不到是不是一个真的缺憾？ 我觉得很难判断，令 prompt 的研究方向笼罩在迷雾之中。
	
	\item 现在考虑图 (\ref{fig:rule-base}) 即 rule base 的 na\"{i}ve 学习算法。 这个算法当然是很慢的，因为要比较两个集合（working memory 和 rule head）的相似性。 假设两个集合的大小固定为 $N$，那需要 $N \times N$ 次的 dot products，而这只是比较了一条 rule.  所有 rules 还要用 softmax 相加。 当 rule base 很大的时候，这个算法似乎不太实际。
	
	\item 图 (\ref{fig:rule-base}) 还有可能出现 旧有的逻辑 rule learning 算法的问题，亦即 ``\textbf{plateau problem}''.  举例来说，用 Prolog 语言写 append 函数：\\
	\code{append(X,Y,Z) :-} \\
	\tab \code{list(X), head(X,X1), tail(X,X2), append(X2,Y,W), cons(X1,W,Z).} \\
	这个 rule 有 5个前提。 当 rule 被学习时，前提被逐个加进去，但 rule 的「得分值」一直是零，直到最后的前提 加进去了，得分才突然升到 100\%.  对于机器学习来说，这情况是很糟的。 而 Transformer 将 rules「扭曲地」缠在一起，这做法会不会反而有利于避免困在 local minima 呢？
	
	\item 
\end{itemize}

How to measure similarity between $(P_i, Q_i)$?
\begin{equation}
\argmin_i \sum_i \min_j \langle P_i, Q_j \rangle
\end{equation}

\color{teal}

\begin{itemize}
	\item 首先留意到，轴心结构 只对 命题层次有用，它的作用不会延伸到 概念原子层次。 然而，有监于 Transformer 也没有充分利用 交换不变性，然而它却因为用了矩阵乘法而变得很有效率，所以从效率的角度看，也有将 轴心结构 延伸到 原子层面的理由。
	
	\item 另一个可以尝试的想法是： 直接 hard-code copying mechanism.  这可以怎样用 Attention 做到？  以前分析过了，copy 并不容易，因为需要 winner takes all.
	
	\item 但纯粹用 Hopfield network 又缺乏了 深度。 但似乎 在 RL 场景下，\textbf{广度} 也是重要的。
	
	\item 其实 只要有 输入/输出 的函数关系，就等价于有 逻辑 rules 的 KB 库。 问题是它用什么方法给出结论。
\end{itemize}

Self-Attention 已经符合我们的要求，但我们想改进它。 主要有两个 idea： 其一是更直接的 copy mechanism，其二是 更细致的 函数 dependence.
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{logic-Transformer-1.png}}}
\end{equation}

\textbf{Copy:}
\begin{itemize}
	\item Copy 不需要什么特别机制，输出就是输入。 但问题是怎样 combine with ``create'' operation.
	\item 当然 可以用大家都熟悉的 softmax： $ \alpha \mbox{ copy} + \beta \mbox{ create} $, where $\alpha, \beta$ are outputs of softmax.
	\item 另一个想法是 content-addressable.  用 table-lookup 的形式，找可执行的 rules。 但有 variable matching 的问题。 
\end{itemize}

\textbf{Create：}
\begin{itemize}
	\item 需要的是怎样的函数？
\end{itemize}

\end{minipage}
\end{preview}


\begin{preview}
\begin{minipage}{\textwidth}

\setlength{\parskip}{0.4\baselineskip}
\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 5}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{逻辑与深度学习}{Logic and Deep Learning}}
\end{textblock*}
\vspace*{0.3cm} 

%我们希望 以代数形式表达 谓词 $\atom$ 的颗粒性。 
在 范畴逻辑 里面有 \textbf{Beck-Chevalley} 条件和 \textbf{Frobenius} 条件，或许是我们所需的对称性？ 但细看之后，发觉还是不能解决问题.....  For completeness，我还是描述一下，没兴趣的可以略过。

首先考虑比较容易明白的 \textbf{Frobenius} 条件。 在逻辑上，它等于说：
\begin{equation}
\exists x. [ \phi \wedge \psi(x) ] \equiv \phi \wedge \exists x. \psi(x).
\end{equation}
由于 经典逻辑 AI 普遍使用 $\forall$ 而忽略 $\exists$，我将上式改写成：
\begin{equation}
\label{eqn:Frobenius-condition}
\forall x. [ \phi \vee \psi(x) ] \equiv \phi \vee \forall x. \psi(x).
\end{equation}
但问题是，(\ref{eqn:Frobenius-condition}) 式的左边和右边，其对应的神经网络 (\ref{fig:logic-symmetry-predicate-level}) 是一样的（看不出有分别）。 也就是说这个差别可能太 subtle 了，它并不影响我们实际 implement 的神经网络。

在逻辑里，任何变量 例如 $x,y$ 等，必须被 $\forall$ 或 $\exists$ quantify，否则不成为合法的句子。 所以 表达 谓词结构的对称性，也很可能要涉及 $\forall$ 或 $\exists$.

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
		
\setlength{\parskip}{0.4\baselineskip}
\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 6}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{逻辑与深度学习}{Logic and Deep Learning}}
\end{textblock*}
\vspace*{0.3cm} 

以前说过，谓词逻辑 带来 \textbf{fibration} 或 \textbf{indexing} 结构。 Beck-Chevalley 和 Frobenius 条件 基本上是说，这 纤维结构 是 ``preserved by re-indexing functors''.

这是 fibration 结构的示意图：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{etale-space.png}}}
\end{equation}
这整个结构 叫 \textbf{bundle}，而 \textbf{sheaf} 是 bundle 加上某个特殊的 拓扑结构。 

在 $(A,f)$ 和 $(B,g)$ 两个 bundle 之上可以定义 \textbf{fibred product} of $A$ and $B$ over $I$, 记作 $A \times_I B$:
\begin{equation}
\label{eqn:fibred-product}
\begin{tikzcd}[sep = 3cm]
A \times B \arrow[r, "q"] \arrow[d, swap, "p"] \arrow[rd, "h"] & B \arrow[d, "g"] \\
A \arrow[r, swap, "f"] & I
\end{tikzcd}
\end{equation}
其中 $h = f \circ p = g \circ q$.  这也是一个 \textbf{pullback}.

\textbf{Beck-Chevalley} 条件是说 下面这幅图 commute： 
\begin{equation}
\label{eqn:Beck-Chevalley}
\begin{tikzcd}[column sep = 3cm]
K \times J \arrow[r, "u \;\times\; id"] \arrow[d, swap, "\pi"] & I \times J \arrow[d, "\pi"] \\
K \arrow[r, swap, "u"] & I
\end{tikzcd}
\end{equation}
其中 $\pi$ 就是代表 量词 $\forall$ 或 $\exists$ 的 投影，它们是 weakening map $\pi^*$ 的伴随映射。 

Beck-Chevalley 条件并不完全是空洞的；它有可能不成立。 有一个反例是 Pitts 提出的： 考虑 $X \times Y$， 其中 $X = Y = \mathbb{N} \cup \{\infty\}$ 亦即 自然数加上 $\infty$ 作为 top element； 但 $Y$ 是用 discrete order，亦即所有 order 都是 =.  $A$ 是 $X \times Y$ 上的关系： $A = \{ (x,y) \in \mathbb{N \times N} \;|\; x \le y \}$.  那么 $\exists y. (x,y) \in A$ 会是整个 $X$ 集合。 如果考虑 DCPO 范畴，我们要求 fibration of Scott-closed subsets (ordered by inclusion) over DCPO.  $\exists y. A$ 的 Scott closure 的条件是 它是一个 lower set closed under directed joins; 而这个 Scott closure 条件似乎不成立，因而导致 图 (\ref{eqn:Beck-Chevalley}) 不 commute.（我对 Scott closure 的细节不太理解）

Lawvere 的工作将 Beck-Chevalley 条件变得更一般化： 「简单」的 $\forall$ 和 $\exists$ 量词 是 weakening functor $\pi^*$（基于笛卡尔积）的伴随函子，但 Lawvere 将它扩充到任何 \textbf{substitution} functor $u^*$.

\end{minipage}
\end{preview}

\end{document}
