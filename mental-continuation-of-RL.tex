\input{../YKY-preamble.tex}
\setmainfont[BoldFont=Alibaba_Sans_Regular.otf,ItalicFont=Alibaba_Sans_Light_Italic.otf]{Alibaba_Sans_Light.otf}
	
\usepackage[active,tightpage]{preview}		% for continuous page(s)
\renewcommand{\PreviewBorder}{0.5cm}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}

\usepackage[absolute,overlay]{textpos}		% for page number on upper left corner

\usepackage{color}
\usepackage{mathtools}
\usepackage[hyperfootnotes=false]{hyperref}

% \usepackage[backend=biber,style=numeric]{biblatex}
% \bibliography{../AGI-book}
% \renewcommand*{\bibfont}{\footnotesize}

\usetikzlibrary{shapes}
\usepackage[export]{adjustbox}				% ??
\usepackage{bm}
\usepackage{verbatim} % for comments
% \usepackage{newtxtext,newtxmath}	% Times New Roman font

% \numberwithin{equation}{subsection}

\newcommand{\underdash}[1]{%
	\tikz[baseline=(toUnderline.base)]{
		\node[inner sep=1pt,outer sep=10pt] (toUnderline) {#1};
		\draw[dashed] ([yshift=-0pt]toUnderline.south west) -- ([yshift=-0pt]toUnderline.south east);
	}%
}%

% Tic-Tac-Toe symbols
% \newcommand{\bO}[0]{\raisebox{-0.2em}{\textbf{O}}}
% \newcommand{\Xb}[0]{\raisebox{-0.2em}{\textbf{X}}}

%\DeclareSymbolFont{symbolsC}{U}{txsyc}{m}{n}
%\DeclareMathSymbol{\strictif}{\mathrel}{symbolsC}{74}
\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
% \setmathfont{Latin Modern Math}

% \newcommand{\highlight}[1]{\colorbox{pink}{$\displaystyle #1$}}

% \newcommand{\emp}[1]{{\color{violet}\textbf{#1}}}
\let\oldtextbf\textbf
\renewcommand{\textbf}[1]{\textcolor{blue}{\oldtextbf{#1}}}

\newcommand*\cryLaughFace{$\vcenter{\hbox{\includegraphics[scale=0.08]{../crying-laughing.jpg}}}$}
\newcommand{\underconst}{\includegraphics[scale=0.5]{../2020/UnderConst.png}}
\newcommand{\KBsymbol}{\vcenter{\hbox{\includegraphics[scale=1]{../KB-symbol.png}}}}
\newcommand{\witness}{\scalebox{0.6}{$\blacksquare$}}
% \newcommand{\Heytingarrow}{\mathrel{-}\mathrel{\triangleright}}
% \providecommand\Heytingarrow{\relbar\joinrel\mathrel{\vcenter{\hbox{\scalebox{0.75}{$\rhd$}}}}}

\begin{document}

\begin{preview}

\cc{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\Large 强化学习 (RL) 的「思维空间延拓」}}
}{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\Large ``Mental-Space Continuation'' \\
		of Reinforcement Learning (RL)}}
}

% \author{YKY} % Your name
\date{\vspace{-2cm}} % Date, can be changed to a custom date

\maketitle

\setcounter{section}{-1}

% (1) Circled page number on upper left corner
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
{\color{red}{\large \textcircled{\small 1}}}
\end{textblock*}

\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\cc{
	在传统 强化学习里，「环境」只包含 physically observable 的外在环境。\\
	我提出将 RL 延续到\textbf{内在的}思维空间。
}{
	In conventional RL, the \textbf{environment} is physically observable.  I propose to extend it to the \textbf{internal} mental space.
}

\cc{
	从传统 RL 的角度： 人伸手拿苹果，苹果是\textbf{奖励}，伸手是\textbf{动作}。\\
	这些都是可以在环境中\textbf{观察}到的：
}{
	From the traditional RL perspective:  an agent reaches out for an apple, the apple is the \textbf{reward}, moving the arm is an \textbf{action}.  These are all \textbf{observables}:
}
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.5]{man-grab-apple.png}}}
\end{equation}

\cc{
	强化学习的基础是 \textbf{Bellman equation}, 它可以看成是一条「\textbf{递归}」的方程：
}{
	The foundation of RL is the \textbf{Bellman equation}.  It can be viewed as a \textbf{recursive} formula:
}
\begin{equation}
\boxed{\mbox{\cc{当前状态}{Current state}}} \quad V(x_0) = \max _a \{ R + \gamma V(x_1)\} \quad \boxed{\mbox{\cc{下一状态}{Next state}}}
\end{equation}
\cc{
	它将 \textbf{终点}状态的\textbf{价值}「反向传递\footnote{注意这不同于神经网络的 back-prop.}」到 终点前一步的状态的价值，就像下棋的情况，可以一直追溯到第一步棋的价值：
}{
	It propagates the final state's reward \textbf{back} to the previous state, and the state before that... just like in a chess game... and so on until the very first move:
}
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{checkmate-sequence.png}}}
\end{equation}

\cc{
	换句话说，获得苹果的价值，反向传递到「伸手取苹果」这动作的价值。 \\
	So far so good. \\
	而同样地，我们可以继续反向传递到决定「伸手取苹果」之前的一连串\textbf{思维}：
}{
	In other words, the reward of getting an apple \textit{back-propagates}\footnote{Note that this is not the same as ``back-prop'' in deep learning.} to the action of reaching out an arm for the apple.  So far so good.  But we can continue this process back to the \textbf{chain of thoughts} that decided to reach for an apple:
}
\begin{equation}
{\color{blue}\mbox{\cc{我肚饿}{hungry}}} \rightarrow {\color{blue}\mbox{\cc{我要找食物}{need to find food}}} \rightarrow {\color{blue}\mbox{\cc{我看见一只苹果}{see an apple}}} \rightarrow {\color{blue}\mbox{\cc{苹果是食物}{apple is food}}} \rightarrow ....
\end{equation}
\cc{
	换句话说，将\textbf{内部}的思维状态「反转」，看成跟\textbf{外部}的状态，是\uline{同等的地位}：
}{
	In other words, we turn our \textbf{internal} mental states ``inside-out'', viewing them \uline{on an equal footing} as \textbf{external} states:
}
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.5]{man-grab-apple-2.png}}}
\end{equation}
\cc{
	而这跟象棋的价值函数的反向传递是\uline{完全一样的}。 换句话说，我们可以用强化学习的算法，学习思维空间的内容，提供了 AGI 严谨的基础。
	}{
	And this is \uline{exactly analogous} to the propagation of rewards in a chess game.  In other words, we can apply techniques of RL to learn the contents of mental space, with a very rigorous foundation.
}

\cc{
	将 外部和内部状态 \textbf{统一处理}的做法，在哲学上也没有问题，因为其实 brain states 也是物理状态，只是肉眼看不到而已。 大脑状态 即是 神经元群的激活状态，它们之间的 transitions 是由神经\textbf{权重}决定的，而这些权重又是由 \textbf{Hebbian learning} 学习的（至少根据我们现时最好的理解）。 
}{
	The approach of unifying internal and external states is philosophically entirely sound, as ``brain states'' are physical states too, and their transitions are learned via \textbf{Hebbian learning}.}

也有论文指出，RL 跟 working memory 的理论有点不协调： RL 里面的「状态」跟 working memory 并不等同。 但根据这新的 思维空间 理论，二者是重合的。

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 2}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{强化学习的「思维空间延拓」}{Mental-Space Continuation of RL}}
\end{textblock*}

\vspace*{0.3cm} 

\subsubsection{Unifying RL and logic}

「思维空间」理论 可以 统一 \textbf{强化学习} 和 \textbf{逻辑 AI}； 两者是在同一空间进行的。 

% 逻辑 包括 rules 的学习 (learning) 和 推导 (inference) 两种算法，跟 深度学习／神经网络 相似（前向推导 和 反向传播学习），也跟 强化学习 相似（选择 actions 和 更新价值函数）。 而 逻辑学习 根据的目标是 \textbf{事实} (truth), 正如 强化学习 根据的目标是 \textbf{奖励} (rewards).

逻辑是一种根据 ``truth'' 的学习算法，有以下比较：

\begin{tabular}{|c|c|c|c|}
	\hline
	& \oldtextbf{学习算法} & \oldtextbf{推导算法} & \oldtextbf{根据什么学习？} \\
	\hline
	\oldtextbf{深度学习} & 学习权重 & 输入 $\rightarrow$ 输出 映射 & 与理想输出之间的 error \\
	\hline
	\oldtextbf{逻辑} & 学习逻辑 rules & 逻辑推导 & 观察到的事实 (truth) \\
	\hline
	\oldtextbf{强化学习} & 更新价值函数 & 选择行动 & 奖励 (reward) \\
	\hline
\end{tabular}

逻辑是没有感情的，在每个状态之下，可以进行很多不同方向的推导； 逻辑本身不能决定选择哪个方向（因为都是真的）。 逻辑推导的每一步 (step) 都是 思维空间中 \textbf{状态}的转移，而 RL 可以给状态 赋予特定的 \textbf{价值} (utility).

在「新古典」AI 时期，我们曾经尝试 给逻辑状态赋予 ``importance'' 或 ``interestingness'' 等测度，现在可以用 RL 理论将二者漂亮地统一起来。

更详细点说，RL 是根据以下的 Bellman 目标而学习的：
\begin{equation}
\max_{\pi} \; \underset{\substack{a_t \;\sim\; \pi(\cdot | s_t) \\ s_{t+1} \;\sim\; p(\cdot | s_t, a_t) }} {\mathbb{E}} \left[ \sum_{t} \gamma^t R(s_t, a_t) \right]
\end{equation}
它包含两个要学习的概率分布： 策略 $\pi$ 和 世界模型 $p$.

$\pi$ 的学习是 RL 课本上大家熟悉的，而 $p$ 的学习是较少讨论的 \textbf{model-based RL}.  如果选择用 逻辑 建立世界模型，那么 $p$ 就是 逻辑推导的过程，亦即 $s_t, a_t \vdash s_{t+1}$.  注意要使用 probabilistic 逻辑，为了得出的结果是概率分布。

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 3}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{强化学习的「思维空间延拓」}{Mental-Space Continuation of RL}}
\end{textblock*}

\vspace*{0.3cm} 

\cc{
	而 强化学习 又是怎样\textbf{学习} 逻辑内容？ \textbf{动作} 就是由一个逻辑状态 转移到另一个 逻辑状态，也可以看成是 由一些命题 \textbf{推导}出 一个新的命题，那就是 逻辑 \textbf{rule}.  我们要在很多 动作（逻辑 rules）之中选取最好的 动作。 换句话说，要在当前状态下可以执行的 rules 之中选取最好的一个或多个 rule(s).  例如：
}{
	So how does RL learn logical content?  An \textbf{action} here is a transition from one logic state (set of propositions) to another, ie, a logic \textbf{rule}.  The agent tries to learn the best actions (= logic rules) among all possible rules that are \textit{applicable} to the current state.  For example:
}
\begin{equation}
\mbox{\cc{我很肚饿}{I'm hungry}} \rightarrow \mbox{\cc{我要吃苹果}{want to eat an apple}}
\end{equation}
\cc{是一条正确的 rule；}{is a good rule;}
\begin{equation}
\mbox{\cc{肚饿}{hungry}} \rightarrow \mbox{\cc{吃网球}{want to eat a tennis ball}}
\end{equation}
\cc{就比较差了。}{is not so good.}

\cc{
	而 强化学习 的好处是： 理论上，它可以在无限的思维空间里 学到非常\textbf{抽象}的 rules，情形就像它在复杂的游戏\textbf{迷宫}里，学到破解的方法。
}{
	The advantage of this RL framework is that it may be able to learn highly \textbf{abstract} rules from the vast mental space, just like it learns to find solutions out of complicated \textbf{mazes} in Atari games.
}

\setcounter{mpfootnote}{1}
\cc{
	在经典 AI 里已有研究过 逻辑 rules 的 combinatorial 搜寻，例如有这种形式的\textbf{搜寻树}： %（但注意这并不等同于 \textbf{RL 状态空间}的形状 \footnote{我有空的时候会再思考一下： RL 思维状态空间 长什么样子，它有什么特性？ 它跟游戏迷宫 有什么相似/不同？}）
}{
	In classical AI, the \textbf{combinatorial} search of logic rules has been studied, with \textbf{search trees} like this:
}
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{induction1.png}}}
\end{equation}

\cc{
思维空间的一个特性是：理论上，\uline{任何思想都有可能推导出任何思想}。 换句话说，任何两点之间都有可能存在一条路径（= 逻辑 rule = 动作）。 例如，如果一个肚饿的人，看见一只网球，那么「圆型的就可以吃」这条 rule 就似乎可以立即解决他的肚饿，直到他真的尝试吃它：
}{
A special characteristic of \textbf{mental space} is that \uline{any thought can potentially be deduced from any other thoughts}.  In other words, any two points in thoughts space may be connected by a \textbf{path} (= logic rule = action).  For example, if a person is hungry and sees a tennis ball, the rule that ``round implies edible'' may satisfy the objective of finding food, only to be punished when one actually eats the object:	
}
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{tennis-ball-edible.png}}}
\end{equation}
\cc{
他得到的负值奖励 会反向传播至 整个\textbf{推导链}。 经过很多次 迭代，系统最终会发现出错的 rule. 简言之，一个初始的逻辑系统可以是任意地「疯狂」的。 事实上，一些天才正是因为有点疯，才发现了其他人想不到的事物。 这是逻辑学习系统必然有的特性，并不只限于我提出的架构。
}{
This negative reward would back-propagate through the \textbf{inference chain}, and after many iterations, the system would eventually find the culprit rule.  In short, a logical system can get ``as crazy as it wants'' without other prior constraints.  Indeed, some geniuses arrive at non-obvious conclusions because they are a bit ``crazy''.  This property of mental space is common to all learning-reasoning systems, not just my architecture.	
}

%{\color{teal}
%NOTES:	

%要将 model 和 policy 分开来考虑。 是不是所有 logic rules 都是 model 的一部分？  Mental states 不能被观察到，会否有问题？ 如果不能观察到 mental states，怎么学习它的 dynamics？ 可能就是需要 build model from partially observable states.  Mental states 基本上完全是 hidden 的，我们能学到它是因为 output state 跟 事实相符 或者有奖励。 而 output state 可以是 real actions or predictions.  
%
%如果 将 mental state 纯粹视为 model 似乎也可以。 那么，model 是由逻辑算法给出，训练 model 的方式 纯粹是根据 逻辑的「后设规则」。 那么这部分似乎完全脱离了 强化学习，而是 经典 AI 的逻辑学习。 
%
%那么，我说过 RL 会帮助 逻辑 rules 的学习，这是错了吗？ 人的思考 的确会改变 逻辑 encode 的知识，而思考的方向是受情感（奖励）影响的。 但如果根据 model learning 的设定，model 的学习完全不受任何「目标」影响。
%
%那怎样才可以有「目的性思考」呢？ 有个目标不能达到，根据传统 RL，需要改变的是 actions 而 不是 model，但奇怪的是 需要改变的其实是 model.  为什么会这样呢？ 因为 model 不是准确的，有了更正确的 model 可以改变行为。 这是不是 RL 跟 WM (working memory) 之间的冲突？  
%
%之前提出过，state 未必完全等同于 working memory, 因为 state 是可以 observe 的。
%
%如果 将 mental states 也看成是 state 的一部分，那是否可以做到 goal-directed thinking?  那是因为 states 可以受 奖励影响....  我 transition to a more valuable state because it is a better direction of thinking.  或者问： 我为什么 好像 drawn towards some direction of thinking?  Under the mental space picture states are attached with utility values.  根据 model 的观点，其实 model 是完全脱离 RL 的独立存在。  Model-based 也应该 配合 mental space picture.  这有矛盾吗？  Mental states 是可观察的，但它的改变却是受我们控制的，而不是外界环境的自身改变。 这就有点奇怪了。 但也可以看成是一种 actions 改变环境的情况。  那么是什么令系统知道某个 transition 是 bad 的呢？  In the real world this is learned as we can observe the transition not occurring.  But in mental space it is entirely up to us whether the transition can occur or not.  So this is not a concern of RL.  It is a concern of logic learning.  The transition can occur if and only if its conclusions accord with observed states.  This algorithm is independent of RL!
%
%Or is it really independent of RL?  Perhaps (mental) state transitions are made by considering rewards / utility.  But this may be in conflict with logic...?  In logic, an inference is made if it is logical (ie, truthful) and is interesting / important.  Perhaps the truthfulness is taken care of by logic, whereas the interestingness / importance is handled by RL?  And if so, this amounts to saying that interestingness / importance coincides with RL's utility.
%
%所以，到此，断定了 working memory 完全等同于 mental states.
%}

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
	
\setlength{\parskip}{0.4\baselineskip}
\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
{\color{red}{\large \textcircled{\small 4}}}
\hspace{8cm}
\color{blue}{\footnotesize \cc{强化学习的「思维空间延拓」}{Mental-Space Continuation of RL}}
\end{textblock*}

\vspace*{0.3cm} 

\subsubsection{The mind as a ``model'' of the world}

\cc{
在传统 强化学习里 有所谓 \textbf{model-based} 方法，而我们在大脑里面的 \textbf{mental model} of the world 其实是同一个概念，但由于「混合状态」的原因而被「压平」了。
}{
In traditional RL, there are \textbf{model-based} methods, and in our brain we build \textbf{mental models} of the world.  These are actually one and the same concept.
}

\cc{
	根据经典逻辑哲学，一个 (脑中的) \textbf{逻辑命题}，\\
	对应于现实世界中 某个\textbf{状态}：
}{
	As is well-known in classical philosophy of logic, a logic proposition (in the mind) corresponds to certain \textbf{states} of the world:
}
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{logic-reality-correspondence.png}}}
\end{equation}
%\cc{
%	而这亦可以推广到，我们知道别人也有 ``other minds'' 的认知：
%}{
%	And this is still un-problematic for understanding ``other minds'':
%}
%\begin{equation}
%\vcenter{\hbox{\includegraphics[scale=0.7]{other-minds.png}}}
%\end{equation}

\setcounter{mpfootnote}{2}
\cc{
	然而，在「混合状态」或 flattened view 观点下，外部世界 和 思维状态 都存在于同一个状态空间，而 思维状态 是 外部世界 的 model 或 ``theory'':\footnote{符号 $T \models M$ 的意思是： $M$ is a \textbf{model} of $T$; $T$ is a \textbf{theory} of $M$.  这是 逻辑\textbf{模型论} 的术语，有严谨的定义。}
}{
	However, in the unified or ``flat'' view, both internal and external states belong to the same state space, within which, the internal states ``model'' the external states:\footnote{The notation $T \models M$ means: $M$ is a \textbf{model} of $T$;  $T$ is a \textbf{theory} of $M$. This is rigorously defined in model theory.}
}
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{model-meta-model.png}}}
\end{equation}
\cc{
	那么，混合状态空间 本身又有没有 theory 呢？ 那可能是某种 \textbf{元逻辑}。 元逻辑 是一种 归纳偏置，或许会在加速学习上有重要作用。
}{
	Does the unified cognitive space has its own ``theory''?  That may be some kind of \textbf{meta-logic}.  This meta-theory is a form of \textbf{inductive bias} that may be important in accelerating learning on the first-order level.
}


\footnotesize
Picture credits:\\
Human figure from www.onlinewebfonts.com licensed by CC BY 3.0\\
Thought bubble created by Catherine Please from the Noun Project
\end{minipage}
\end{preview}
\end{document}
