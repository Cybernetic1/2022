\input{../YKY-preamble.tex}

\usepackage{color}
\usepackage{mathtools}
\usepackage{hyperref}

\usepackage[backend=biber,style=numeric]{biblatex}
\bibliography{../AGI-book}
\renewcommand*{\bibfont}{\footnotesize}

\usepackage{graphicx} % Allows including images
\usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage[export]{adjustbox}% http://ctan.org/pkg/adjustbox
\usepackage{bm}
\usepackage{verbatim} % for comments
\usepackage[most]{tcolorbox}
% \usepackage{newtxtext,newtxmath}	% Times New Roman font

% \numberwithin{equation}{subsection}

\newcommand{\underdash}[1]{%
	\tikz[baseline=(toUnderline.base)]{
		\node[inner sep=1pt,outer sep=10pt] (toUnderline) {#1};
		\draw[dashed] ([yshift=-0pt]toUnderline.south west) -- ([yshift=-0pt]toUnderline.south east);
	}%
}%

\newcommand{\bO}[0]{\raisebox{-0.2em}{\textbf{O}}}
\newcommand{\Xb}[0]{\raisebox{-0.2em}{\textbf{X}}}

%\DeclareSymbolFont{symbolsC}{U}{txsyc}{m}{n}
%\DeclareMathSymbol{\strictif}{\mathrel}{symbolsC}{74}

\newcommand{\highlight}[1]{\colorbox{pink}{$\displaystyle #1$}}

\newcommand{\emp}[1]{{\color{violet}\textbf{#1}}}
\newcommand*\confoundFace{$\vcenter{\hbox{\includegraphics[scale=0.2]{../2020/../confounded-face.jpg}}}$}
\newcommand{\underconst}{\includegraphics[scale=0.5]{../2020/UnderConst.png}}
\newcommand{\KBsymbol}{\vcenter{\hbox{\includegraphics[scale=1]{../KB-symbol.png}}}}
\newcommand{\witness}{\scalebox{0.6}{$\blacksquare$}}
% \newcommand{\Heytingarrow}{\mathrel{-}\mathrel{\triangleright}}
\providecommand\Heytingarrow{\relbar\joinrel\mathrel{\vcenter{\hbox{\scalebox{0.75}{$\rhd$}}}}}

\begin{document}

\title{\bfseries\color{blue}{\Huge AGI standard model}\\ --- trying to establish a consensus}
\author{YKY} % Your name
%\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
%{
%Independent researcher, Hong Kong \\ % Your institution for the title page
%\medskip
%\textit{generic.intelligence@gmail.com} % Your email address
%}
\date{\today} % Date, can be changed to a custom date

\maketitle

\begin{abstract}
The essence of the standard model is just to identify a \textbf{Working Memory} as the ``state'' of the AGI system.  This establishes connections between reinforcement learning, Turing machines, neural Turing machines, self-attention, Transformers, and even the biological brain.  Lastly, we describe two sub-problems: abductive reasoning and making assumptions, that also seem essential to AGI systems.
\end{abstract}

% \vspace*{0.5cm}
% 多谢 支持 \smiley

\setcounter{section}{-1}
\section{Introduction}

The ``standard model'' is a way of thinking, that may help us better understand the general theory of AGI systems.

The essence of the standard model is just to identify a \textbf{Working Memory} as the ``state'' of the AGI system.

One benefit of our theory is that it relates Transformers / BERT / GPT to AGI systems.  These language models are phenomenally intelligent, yet many people criticize them as not ``truly'' intelligent.  The standard model suggests that they are indeed linked to AGI.

\section{Reinforcement learning}

This is the simplest form of a \textbf{dynamical system}:
\begin{equation}
\begin{tikzpicture}[->]
\node[rectangle,draw,label=0:{{$\quad =$} ``working memory"}] (C) {state $x$};
\path (C) edge [out=30,in=150,looseness=4,thick] node [above] {transition function $F$} (C);
\end{tikzpicture}
\end{equation}
When we add a ``control'' or ``action'' variable $a$ to it, it becomes the most basic  \textbf{control system}:
\begin{equation}
\begin{tikzpicture}[->]
\node[rectangle,draw] (C) {$x$};
\path (C) edge [out=30,in=150,looseness=7,thick] node [above] {$F(x, a)$} (C);
\end{tikzpicture}
\end{equation}
which is the setting for Dynamic Programming or \textbf{Reinforcement Learning}.  The optimal solution for such systems is governed by the \textbf{Hamilton-Jacobi-Bellman equation}:
\begin{equation}
V^*_t = \max_a \mathbb{E} [ R + V^*_{t+1} ]
\end{equation}


Here I want to ask a question:  reinforcement learning is not 

Recently, Yann LeCun's \textbf{Energy-Based Models} offers a way to circumvent the problem of learning probability distributions over actions, when the action space is hugely high-dimensional.  This seems to be an important step towards AGI systems.

I call this the ``standard model'' because of the extreme simplicity of this setup, and that I don't know of other alternative models that deviate much from it.

%\section{The standard model in relation to other systems}

The following diagram shows how the standard model relates to several other important areas, so we can reap profits from their interactions:

\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.8]{AGI-standard-model.png}}}
\end{equation}

\section{Neural Turing Machines and Transformers}

The \textbf{attention mechanism} was first proposed in the ``\textbf{Neural Turing Machine}''  paper by Graves \textit{et al} [2014].

Recall that a Turing machine is a \textbf{Finite State Machine} augmented with a \textbf{Memory Tape}:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.9]{Turing-machine.png}}}
\end{equation}

In Neural Turing Machines, Graves \textit{et al} proposed the attention mechanism for an RNN ``Controller'' (playing the role of the Finite State Machine) to read and write from a \textbf{Memory Matrix} (the tape), using a content-based addressing method.

The Memory Matrix $M$ consists of $N$ items, each of constant size.  \textit{The discreteness of the address would introduce discontinuities in gradients of the output}, hence we need an \textbf{Attention Vector} to focus on a specific location in the memory matrix $M$.

The Attention Vector $\vec{a}$ is calculated via the following formula, familiar to students of the Transformer: 
\begin{equation}
\vec{a} = \mathrm{soft} \max_i \{  \mathcal{D}(K, M_i) \}
\end{equation}
where $D()$ is a similarity measure between the key $K$ and memory item $M_i$.  The key $K$ is emitted by the Controller as the value that it is looking for.

This then evolved into the \textbf{Self Attention} mechanism used in all Transformers.  Now let us refresh with this diagram illustrating Self-Attention (redrawn from a blog article on the web):

\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.9]{self-attention-2.png}}}
\end{equation}

The research done by Olah \textit{et al}, in their 2021 paper \textit{\uline{A Mathematical Framework for Transformer Circuits}}, is very helpful towards understanding Transformers and Self-Attention.  The paper is technically quite challenging, but thanks to the guidance of professor Xiao Da from Beijing I was able to understand the main ideas.  Here I offer some pointers to help others understand the paper, without explaining it in full details.

The Self Attention $A$ is calculated by matrix operations followed by \textbf{softmax}, and thus is non-linear.  However, when $A$ is held constant, the Transformer becomes a linear operation that can be expressed as \textbf{tensor products}, for example:

\begin{equation}
\label{eqn:circuit1}
\vcenter{\hbox{\includegraphics[scale=0.4]{Transformer-circuit-1.png}}}
\end{equation}

For example, to understand the equation for a Transformer ``Head'':
\begin{equation}
h(x) = (\text{Id} \otimes W_O) \cdot (A \otimes \text{Id}) \cdot (\text{Id} \otimes W_V) \cdot x
\end{equation}
it is helpful to know that $x$ is a \textbf{matrix} with dimensions $[n_{\mathrm{context}}, d_{\mathrm{model}}]$, ie, one dimension being the number of tokens and the other dimension representing the token's vector.  When a tensor such as $U \otimes V$ acts on $x$, $U$ acts on the ``position'' dimension of the matrix while $V$ acts on the ``vector'' dimension.  So the above equation can be collapsed to:
\begin{equation}
h(x) = (A \otimes W_O W_V) \cdot x .
\end{equation}
Thus tensor products are very convenient for expressing such operations.

\begin{tcolorbox}[breakable, parbox=false, fonttitle=\bfseries, title=Tensor product]
	
	Recall that a \textbf{linear} map is defined between two vector spaces:
	\begin{equation}
	F: U \rightarrow V .
	\end{equation}
	Similarly one can define a \textbf{bi-linear} map:
	\begin{equation}
	\Phi : V \times W \rightarrow U
	\end{equation}
	that is linear in both its first and second arguments.  The tensor product $\otimes$ is the \textbf{universal} multi-linear map.
	
	The tensor product generalizes to the mathematically important \textbf{monoidal categories}.  The latter captures the associativity (and optionally commutativity, for which we have symmetric monoidal categories), but the linearity is abstracted away.
	
	Monoidal categories can be depicted by \textbf{String Diagrams}.  There are also \textbf{Neural String Diagrams} that describe the structure of neural networks.  For example, the Transformer layer is \textbf{equivariant} in its inputs.

\end{tcolorbox}

In the Transformer, we can see \textbf{Input Values} flowing through some ``circuits'' defined by matrices to yield the \textbf{Output Values}:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.4]{Transformer-circuit-2.png}}}
\end{equation}

Some Attention Heads merely perform ``\textbf{copying}'' of a vector from a previous layer to the next.  If we spell this out as a linear equation, $W \vec{x} = \lambda \vec{x}$, where the transformation $W$ merely results in a \textbf{scaling} of the vector $\vec{x}$.  This is just an \textbf{eigenvector} equation.  So if we plot the eigenvalues of the transformation matrices on the complex plane, the ``copy'' maps would have eigenvalues close to positive real numbers.  The results shows that such ``copy'' maps are ubiquitous in Attention Heads (here 10 out of 12 maps are positive):
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.47]{eigenvalue-plot-of-Attention-Heads.png}}}
\end{equation}
Such Attention Heads may be interpreted as \textbf{universally-quantified} ($\forall$) logic formulas.  For example when we say ``all men are mortal'':
\begin{equation}
\forall x. \; \mbox{Human}(x) \Rightarrow \mbox{Mortal}(x)
\end{equation}
any object instantiated as $x$ (eg. ``Socrates'') would have to be \textbf{copied} from the LHS to the RHS.

\section{Relation to the biological brain}

There are two distinct aspects in the brain:
\begin{itemize}
	\item \textbf{Short-term} or Working Memory is the \textbf{electric activation} of neuronal populations.
	\item \textbf{Long-term} memory is stored as \textbf{synaptic strengths}, established by synaptic formation and strengthening.  The transfer from STM to LTM is called \textbf{memory consolidation}.
\end{itemize}

One theory has it that the prefrontal cortex maintains a number of ``thoughts'' with sub-populations or, perhaps, with \textbf{micro-columns}.  These activated sub-populations are in competition with each other, through \textbf{lateral inhibition}.  The thought(s) that win are the thoughts we retain -- they ``make sense''.

\subsection{How does symbolic logic emerge in the brain?}

If a room of people see a cat enter the room, one person will say ``There's a cat in the room!'' but afterwards it would be \textbf{redundant} for others to say exactly the same thing.  Likewise, in a neural network, if two output features both identify ``cat'' then they are redundant, a waste of resources.  So it is more efficient for one feature vector to move away to a new location in \textbf{feature space}:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{disentangled-features.png}}}
\end{equation}
The result is the emergence of \textbf{disentangled features}.  There is now a lot of research papers on this topic;  Personally I first learned of this from Marta Garnelo and Murray Shanahan's paper  \parencite{Shanahan2019}.  We can think of this as a first step of \textbf{symbolization}, in which objects are recognized by symbols.

In the cortex, neuronal populations are organized into ``columns'', with \textbf{lateral inhibition}  among themselves.  When one population is activated, it suppresses the activation of nearby populations.  This is likely to be the mechanism that enables disentangled features to emerge:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{lateral-inhibition.png}}}
\end{equation}
It is remarkable that the \textbf{softmax} in the Transformer / Self-Attention seems to be an abstract implementation of this winner-takes-all \textbf{selection} mechanism.

Moreover, the cortex is organized into \textbf{layers} with widespread recurrent (ie, forward and backward) connections \footnote{More accurately, there exist two distinct structures: the cortex has a 6-layer structure which has recurrent connections within it;  and each cortical area has bi-directional connections to and from other areas (which may have hierarchical relations among themselves).  I have sort of glossed over this level of details.}:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{cortex-back-projections.png}}}
\label{fig:cortex-back-projections}
\end{equation}
This bi-directional architecture may be applicable to AGI architecture (see also \S\ref{sec:abductive-reasoning} on abductive reasoning), possibly replacing the current uni-directional model of feed-forward networks and the back-propagation algorithm.  As is well-known, the brain does not use back-prop.  The bi-directional innervation is a very significant brain architectural feature that has not yet been incorporated into current deep learning techniques.

If we consider relations between objects, for example, ``spoon inside a glass'', this too can emerge out of disentanglement of features, because it is a very \textbf{economical} / efficient representation of a complex scene:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{spoon-in-glass-movie.png}}}
\end{equation}
Every human can recognize this as ``putting a spoon into a glass'', a symbolic representation.  Many researchers may have under-estimated how much the brain uses symbolic reasoning, and my proposal is that AGI can be based entirely on it.  

One remaining question is how to represent symbolic data in a ``neural'' manner.  A general form of symbolic data may be as a \textbf{tree}.  Taking inspiration from the cortex (\ref{fig:cortex-back-projections}), we may perhaps represent the tree / symbolic data as hierarchically organized neural \textbf{feature vectors}:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{hierarchical-features.png}}}
\end{equation}

Remember that in the Transformer, symbols are organized as \textbf{sequences}, for example: ``spoon $\cdot$ inside $\cdot$ glass.''  It may be desirable for AGI to have multiple levels of features, such as ``spoon'' and ``glass'' on a lower level, and ``inside'' on a higher level.

Juxtaposed side by side, the Transformer and the cortex seem to have many similarities:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{Transformer-cortex-correspondence.png}}}
\end{equation}
Softmax corresponds to lateral inhibition.  The Transformer has many layers because it \textbf{unfolds} along the time axis the training of a recurrent network --- part of the reason why the Transformer is very efficient.  Each hidden layer of the Transformer can be construed as a ``stage'' of logical inference:
\begin{equation}
\mbox{input} \vdash \mbox{stage}_1 \vdash \mbox{stage}_2 \vdash .... \vdash \mbox{output}.
\end{equation}

Also recall that our reinforcement learning model consists of just the state and its transition function:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{cortex-with-transition-function.png}}}
\end{equation}
Based on this understanding, we need to figure out how to design the next version of Transformer and incorporate it into our AGI architecture....

\section{Abductive reasoning}
\label{sec:abductive-reasoning}

Abduction has been relatively neglected in AGI research, which had focused on forward inference.  Recently there is a call to study this important aspect.

In logic, abduction means finding the \textbf{explanation} for some known facts.  An explanation $E$ is simply some propositions that imply the known fact $F$, ie, $E \Rightarrow F$.

For example, why do we think a certain actress, say Marilyn Monroe, is ``sexy'' \footnote{I use this example because ``sexy'' is a kind of high-level concept and we apply this concept commonly when we meet other people.  Every one can relate to the ``reasons'' why we think someone is sexy.} ?  That's because we recognize she has some features (visual or otherwise, no need to enumerate them explicitly) that we consider sexy.  So, $E_1 \wedge E_2 \wedge .... \Rightarrow \mathrm{Sexy}$.  Those conditions \textbf{imply} she is sexy, and they are the \textbf{explanation} for her sexiness.

Why is abduction important?  For example, when a waitress says ``The Ham Sandwich left a big tip'', \textit{Ham Sandwich} here refers to the customer who ordered it (an example of metonymy).  The AI knows the plain facts such as that someone ordered a ham sandwich, and then it abduces that the most likely \textbf{interpretation} of the phrase ``Ham Sandwich'' is as the person associated with it.  This is the basis of \textbf{Abductive Interpretation of Natural Language} proposed by Jerry Hobbs:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{Jerry-Hobbs.png}}}
\end{equation}

So abductive reasoning is basically just \textbf{bidirectional} inference.

When a system has both forward and backward connections, it forms a loop and its dynamics is likely to produce ``\textbf{resonance}''.  This harks back to the ART (\textbf{Adaptive Resonance Theory}) proposed by Grossberg and Carpenter beginning in the 1980s.

Such resonance behavior can be viewed as the system seeking to minimize an energy, ie, trying to find the ``best explanation'' to a set of facts.  

This is also corroberated by neuroscientific evidence:  areas in the cerebral cortex are replete with both forward- as well as \textbf{back-projections}, as depicted in diagram (\ref{fig:cortex-back-projections}).  We can further abstract this with the following diagram, where $F$ and $G$ are not functions but \textbf{optimization constraints}:
\begin{equation}
\begin{tikzcd}
\vec{y} \arrow[d, bend left, "G"]  \\
\vec{x} \arrow[u, bend left, "F"]
\end{tikzcd}
\end{equation}
If the input $\vec{x}$ produces the output $\vec{y}$ after some iterations, then it is likely that the output $\vec{y}$ would produce $\vec{x}$ in the \textbf{inverse} direction.  In other words, we have a \textbf{neural} mechanism that implements a function $f$ and its inverse $f^{-1}$.  The significance of this (from the \textbf{learning} point of view) is that we only need to learn the function $f$ and we get $f^{-1}$ \textbf{for free}.

In logic, if forward inference is denoted as $\vdash_{\KBsymbol}$, where $\KBsymbol$ is a set of logic rules, then abduction is $(\vdash_{\KBsymbol})^{-1}$.  Abductive interpretation is basically a \textbf{constraint-satisfaction} process that uses inference rules in both directions.

\section{Dealing with assumptions}

Example of an assumption:  ``If I play move $x$ now, I can checkmate in 3 moves".

\begin{tcolorbox}[breakable, parbox=false, fonttitle=\bfseries, title=Tic Tac Toe example]
	
	Assume the current board is
	\scalebox{0.5}{
		\begin{tabular}{c|c|c}
			&     & \bO \\ \hline
			\bO & \Xb &     \\ \hline
			\Xb &     & 
	\end{tabular}} and it's \Xb's turn to play.
	
	\Xb \ can do the ``double fork'' by playing
	\scalebox{0.5}{
		\begin{tabular}{c|c|c}
			&     & \bO \\ \hline
			\bO & \Xb &     \\ \hline
			\Xb &     & {\color{red}\Xb}
	\end{tabular}}.
	
	But how can an AGI know (or prove) this?
	
	If the current board is 
	\scalebox{0.5}{
		\begin{tabular}{c|c|c}
			&     & \bO \\ \hline
			\bO & \Xb &     \\ \hline
			\Xb &     & \Xb
	\end{tabular}}  then a double fork exists.  We need a predicate to detect double forks.
	
	We need to reason that even if \ \bO \ plays the ``blocking'' move
	\scalebox{0.5}{
		\begin{tabular}{c|c|c}
			{\color{red}\bO} &     & \bO \\ \hline
			\bO & \Xb &     \\ \hline
			\Xb &     & \Xb
	\end{tabular}}  , \Xb \ can still win.
	
	We can easily express the conditions for \Xb-can-win, but the difficult part is to make the assumption in {\color{red}red}, in other words:
	\begin{equation}
	\mbox{{\color{red}red-move}} \Rightarrow \mbox{\Xb-can-win}
	\end{equation}
	The difficulty lies in that the LHS is \textbf{not true} under the current facts.  This conditional statement must be proven by, first, assuming the LHS, and then deriving the RHS.  Then the assumption is \textbf{discharged} and the conditional statement is proven, via the ($\Rightarrow$ I) rule.
	
\end{tcolorbox}

In the old days, in logic-based AI, assumptions are handled with \textbf{Truth Maintenance Systems} that keep track of inference traces symbolically.  These systems can get quite complicated with the need to track multiple assumptions.  For example, when we plan a bank robbery, we need to consider many possible forking scenarios.  

\begin{tcolorbox}[breakable, parbox=false, fonttitle=\bfseries, title=Curry-Howard Correspondence]

	This is the assumption rule (Ax) in proof theory:
	\begin{equation}
	\Gamma, \phi \vdash \phi \qquad (\mbox{Ax})
	\end{equation}
	An example instance is:
	\begin{equation}
	x : \phi, \; y : \psi \vdash x : \phi \qquad (\mbox{Ax})
	\end{equation}
	Here $x$ is a \textbf{$\lambda$-variable} of type $\phi$.  In general, assumptions are $\lambda$-variables.
	
	This is the ($\rightarrow$I) ``introduction'' rule:
	\begin{equation}
	\frac{ \Gamma, x:\phi \vdash M:\psi }
	{ \Gamma \vdash \lambda x. M : \phi \rightarrow \psi }
	\qquad (\rightarrow I)
	\label{eqn:implication-intro}
	\end{equation}
	Thus, \textbf{discharging} an assumption (ie, applying the $\rightarrow$I rule) results in a \textbf{$\lambda$-term}.
	
	Suppose $M, N$ are proofs of $M: \phi \rightarrow \psi$ and $N: \phi$.  Then the proof of $\psi$ would be the \textbf{application} of $M$ to $N$, denoted as $@(M, N)$ or simply $M N$.  This is \textit{modus ponens}.
	
	Similarly, for the rule ($\rightarrow$I) in (\ref{eqn:implication-intro}), the proof of $\psi$ can be denoted as $\#(x, M)$ or... why not try $\lambda x. M$?  Yes, what we get is the lambda-notation.  In the words of another author:  ``\uline{The similarity is not intended and not accidental.  It is unavoidable}.''  \footnote{Lectures on the Curry-Howard Isomorphism [S\o{}rensen and Urzyczyn 2006], p.56} The proof of an implication is a construction, which, according to the Brouwer-Heyting-Kolmogorov interpretation, is a \textbf{function}.

\end{tcolorbox}

An AGI needs the ability to place an implication $\phi \rightarrow \psi$ into working memory, and to prove it using the ($\rightarrow$ I) rule, ie, by making an assumption.

\textbf{Algorithm:}  Put the assumption $A$ in Working Memory.  Make inferences, marking all conclusions with the prefix $A \rightarrow *$.  When enough conclusions are obtained, remove the assumption $A$ from Working Memory.

\end{document}
